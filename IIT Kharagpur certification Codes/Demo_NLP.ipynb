{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OBT39APNBeF"
      },
      "outputs": [],
      "source": [
        "import re                           #Imports the regular expression module, which provides tools for text pattern matching and manipulation (used here to remove punctuation).\n",
        "import nltk                         #Imports the Natural Language Toolkit (NLTK) to provide tools for tokenization, lemmatization, and more.\n",
        "from nltk.corpus import stopwords   #Imports the stopwords corpus from NLTK, which contains a list of common words (such as \"the\", \"and\", \"in\") that are often removed in text preprocessing because they do not carry significant meaning.\n",
        "from nltk.tokenize import word_tokenize   #Imports the word_tokenize function from NLTK, which splits a text into individual words (tokens).\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources (run only once)\n",
        "nltk.download('punkt')       #Downloads the Punkt tokenizer models, which are necessary for tokenization (splitting text into words or sentences).\n",
        "nltk.download('stopwords')   #Downloads the list of stopwords in various languages, including English. This list is used to filter out common, non-informative words.\n",
        "nltk.download('wordnet')     #Downloads the WordNet lexicon, a large lexical database of English, which is used for lemmatization. It helps the lemmatizer determine the base form of words.\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4gK4TC6N1BU",
        "outputId": "bfba8ef7-09df-4f72-82ef-c4016409f2df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text\n",
        "text = \"This is an example sentence, to demonstrate text preprocessing! We'll clean it and tokenize.\"\n",
        "\n",
        "#The text contains punctuation, capital letters, and stopwords, making it a good candidate for cleaning.\n",
        "\n",
        "# Function for text preprocessing\n",
        "def preprocess_text(text):           #Defines a function preprocess_text that takes a string text as input and performs several preprocessing steps (like lowercasing, tokenization, etc.).\n",
        "    # 1. Lowercase the text\n",
        "    text = text.lower()              #Converts the entire text to lowercase to ensure uniformity, as text processing often ignores case sensitivity. For example, \"Text\" and \"text\" should be treated as the same word.\n",
        "\n",
        "    # 2. Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)      #Uses the re.sub function to remove punctuation and special characters. The regular expression r'[^\\w\\s]' matches any character that is not a word character (\\w) or a whitespace (\\s). This removes things like commas, periods, and exclamation marks.\n",
        "\n",
        "    # 3. Tokenize the text\n",
        "    tokens = word_tokenize(text)        #This splits the text into individual words, resulting in a list of tokens.\n",
        "\n",
        "    # 4. Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))   #Loads a set of stopwords from the NLTK library for the English language.\n",
        "    tokens = [word for word in tokens if word not in stop_words]   #Filters out the stopwords from the tokenized text.\n",
        "\n",
        "    # 5. Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()  #Creates an instance of the WordNetLemmatizer.\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]    #Applies lemmatization to each token in the list.\n",
        "\n",
        "    return tokens      #Returns the list of preprocessed tokens (cleaned, tokenized, stopword-free, and lemmatized) as the output of the function."
      ],
      "metadata": {
        "id": "kL4Cnd5DN_CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the example text\n",
        "cleaned_text = preprocess_text(text)     #Calls the preprocess_text function on the example text. This applies all the preprocessing steps defined earlier, resulting in a cleaned list of tokens.\n",
        "print(\"Cleaned Tokens:\", cleaned_text)   #Prints the cleaned tokens to the console, showing the final result of the preprocessing steps."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFvPhAOKOkhg",
        "outputId": "aca24976-e995-4bd0-e9d4-58b68255386a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Tokens: ['example', 'sentence', 'demonstrate', 'text', 'preprocessing', 'well', 'clean', 'tokenize']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy                         #This imports the spaCy library, which is an advanced NLP library for Python. It provides tools for tokenization, part-of-speech tagging, named entity recognition, dependency parsing, and more.\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")    #This loads the small English language model (en_core_web_sm) provided by spaCy. en_core_web_sm is the smallest English model, optimized for speed rather than accuracy.\n",
        "\n",
        "# Sample text\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion. Steve Jobs founded Apple in 1976.\"\n",
        "\n",
        "#It contains a few named entities (like \"Apple\" and \"Steve Jobs\"), as well as some financial information (like \"$1 billion\") and dates.\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(text)     #nlp(text) processes the input text and returns a Doc object, which is a container for the processed text.\n",
        "\n",
        "# Part-of-Speech (POS) tagging\n",
        "print(\"Part-of-Speech Tagging:\")  #Part-of-speech (POS) tagging is the task of assigning a grammatical category (like noun, verb, adjective) to each token in the text.\n",
        "for token in doc:\n",
        "    print(f\"{token.text}: {token.pos_} ({token.tag_})\")  #token.text: This gives the original text of each token (word). token.pos_: This gives the coarse-grained part-of-speech label (e.g., NOUN, VERB, ADJ). token.tag_: This provides a more fine-grained POS tag (e.g., NN for singular noun, VBD for past-tense verb, etc.).\n",
        "\n",
        "# Named Entity Recognition (NER)\n",
        "print(\"\\nNamed Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text}: {ent.label_} ({spacy.explain(ent.label_)})\") #doc.ents: This contains a list of all the named entities detected in the processed text. ent.text: This gives the text of the named entity (e.g., \"Apple\", \"$1 billion\"). ent.label_: This gives the label for the entity (e.g., ORG for organizations, GPE for geopolitical entities, DATE for dates, MONEY for monetary values). spacy.explain(ent.label_): This provides an explanation of the entity label.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eORoeEXeO6Km",
        "outputId": "23ab3177-ec4a-4289-f85b-fdc943c4cf47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part-of-Speech Tagging:\n",
            "Apple: PROPN (NNP)\n",
            "is: AUX (VBZ)\n",
            "looking: VERB (VBG)\n",
            "at: ADP (IN)\n",
            "buying: VERB (VBG)\n",
            "U.K.: PROPN (NNP)\n",
            "startup: VERB (VBD)\n",
            "for: ADP (IN)\n",
            "$: SYM ($)\n",
            "1: NUM (CD)\n",
            "billion: NUM (CD)\n",
            ".: PUNCT (.)\n",
            "Steve: PROPN (NNP)\n",
            "Jobs: PROPN (NNP)\n",
            "founded: VERB (VBD)\n",
            "Apple: PROPN (NNP)\n",
            "in: ADP (IN)\n",
            "1976: NUM (CD)\n",
            ".: PUNCT (.)\n",
            "\n",
            "Named Entities:\n",
            "Apple: ORG (Companies, agencies, institutions, etc.)\n",
            "U.K.: GPE (Countries, cities, states)\n",
            "$1 billion: MONEY (Monetary values, including unit)\n",
            "Steve Jobs: PERSON (People, including fictional)\n",
            "Apple: ORG (Companies, agencies, institutions, etc.)\n",
            "1976: DATE (Absolute or relative dates or periods)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Sample text\n",
        "text = \"I absolutely love this product! It's fantastic and works like a charm.\"\n",
        "\n",
        "# Create a TextBlob object\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Perform sentiment analysis\n",
        "sentiment = blob.sentiment\n",
        "\n",
        "# Output sentiment polarity and subjectivity\n",
        "print(f\"Polarity: {sentiment.polarity}\")  # Polarity: -1 (negative) to 1 (positive)\n",
        "print(f\"Subjectivity: {sentiment.subjectivity}\")  # Subjectivity: 0 (objective) to 1 (subjective)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzp-CiQZPS1_",
        "outputId": "de4c85ca-aeeb-4286-9f42-24721fe0c3e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polarity: 0.5125\n",
            "Subjectivity: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "import gensim                               #Imports the gensim library, which is a popular Python library for topic modeling, document similarity, and vector space modeling. It includes the LDA model, which weâ€™ll use for topic modeling.\n",
        "from gensim import corpora                  #Imports the corpora module from Gensim, which provides utilities for handling a corpus of documents. It includes methods for creating a dictionary (mapping words to unique IDs) and for creating document-term matrices.\n",
        "from gensim.models import LdaModel          #Imports the LdaModel class from Gensim. This class is used for training an LDA model on a corpus to discover topics within a collection of documents.\n",
        "from nltk.corpus import stopwords           #Imports the stopwords corpus from NLTK (Natural Language Toolkit), which contains a list of common words (e.g., \"the\", \"is\", \"and\") that are often removed from text during preprocessing.\n",
        "from nltk.tokenize import word_tokenize     #Imports the word_tokenize function from NLTK, which is used for splitting a sentence into individual words or tokens.\n",
        "import nltk             #Imports the main NLTK library to access other utilities like stopwords and tokenizers.\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Download NLTK stopwords (run only once)\n",
        "nltk.download('punkt')       #Downloads the Punkt tokenizer models, which are necessary for word tokenization (splitting sentences into words).\n",
        "nltk.download('stopwords')   #Downloads the stopwords list, which contains a set of common words in English that are typically removed from text before processing.\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"Artificial intelligence is transforming the technology industry.\",\n",
        "    \"Machine learning and AI are shaping the future of automation.\",\n",
        "    \"Deep learning algorithms are a subset of machine learning.\",\n",
        "    \"Quantum computing will revolutionize industries like AI.\",\n",
        "    \"Healthcare is benefiting from AI and machine learning advances.\",\n",
        "]\n",
        "\n",
        "#This defines a list of sample documents (sentences) that will be used for topic modeling. These sentences are focused on topics related to artificial intelligence (AI) and machine learning (ML).\n",
        "\n",
        "# Preprocess the documents\n",
        "def preprocess(doc):                                    #Defines a function that preprocesses a document (sentence) to prepare it for modeling by removing stopwords and non-alphabetic words.\n",
        "    stop_words = set(stopwords.words('english'))        #Loads the set of English stopwords from NLTK into the stop_words variable. These are words like \"and\", \"the\", \"is\", etc., that generally do not carry important meaning for topic modeling.\n",
        "    tokens = word_tokenize(doc.lower())                 #Tokenizes the input doc (document) into individual words (tokens) and converts all words to lowercase using .lower() to ensure uniformity (e.g., \"AI\" and \"ai\" will be treated as the same).\n",
        "    return [word for word in tokens if word.isalpha() and word not in stop_words]    #Filters out any tokens that are non-alphabetic (such as punctuation or numbers) and any stopwords. It returns a list of meaningful words (tokens).\n",
        "\n",
        "processed_docs = [preprocess(doc) for doc in documents]               #Applies the preprocess() function to each document in the documents list. This results in a list of tokenized, lowercased, stopword-free words for each document.\n",
        "\n",
        "# Create a dictionary and document-term matrix\n",
        "dictionary = corpora.Dictionary(processed_docs)                          #Creates a dictionary using the processed documents. The dictionary maps each unique word (token) to a unique ID. This is an essential step before building a document-term matrix (DTM).\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in processed_docs]    #Converts each preprocessed document into a bag-of-words representation using dictionary.doc2bow(). The doc2bow function converts each document into a list of tuples, where each tuple represents a word ID and its frequency in the document.\n",
        "\n",
        "# Train the LDA model (specifying 2 topics)\n",
        "lda_model = LdaModel(doc_term_matrix, num_topics=2, id2word=dictionary, passes=15)   #Specifies that the model should discover 2 topics from the documents. id2word=dictionary: The dictionary created earlier is passed to the model to help interpret word IDs.. passes=15: Specifies the number of passes (iterations) over the entire corpus to optimize the model. More passes generally result in better topic quality but take more time.\n",
        "\n",
        "# Print the topics with associated words\n",
        "print(\"Topics discovered by LDA:\")\n",
        "topics = lda_model.print_topics(num_words=5)    #Prints the top 5 words for each discovered topic. This allows you to understand what each topic is about based on the most common words in the topic.\n",
        "for topic in topics:\n",
        "    print(topic)        #Iterates over the topics and prints them out. Each topic consists of a list of words that are highly associated with that topic.\n",
        "\n",
        "# Document similarity (clustering example)\n",
        "doc1_bow = dictionary.doc2bow(preprocess(\"AI and machine learning are advancing rapidly\"))   #Preprocesses the new document, converts it into a bag-of-words format using the dictionary, and stores it in doc1_bow.\n",
        "doc2_bow = dictionary.doc2bow(preprocess(\"Healthcare is benefiting from AI advances\"))       #Preprocesses and converts the second document into a bag-of-words representation, storing it in doc2_bow.\n",
        "\n",
        "similarity = gensim.matutils.cossim(doc1_bow, doc2_bow)   #Computes the cosine similarity between the two document vectors (doc1_bow and doc2_bow). Cosine similarity is a measure of similarity between two vectors based on the cosine of the angle between them. A higher cosine value indicates more similarity.\n",
        "print(\"\\nDocument Similarity (cosine):\", similarity)   #Prints the cosine similarity value between the two documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4NpkiwkPpPx",
        "outputId": "add88e4d-ac5f-40eb-d539-91877592ef50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics discovered by LDA:\n",
            "(0, '0.115*\"ai\" + 0.065*\"computing\" + 0.065*\"industries\" + 0.065*\"quantum\" + 0.065*\"like\"')\n",
            "(1, '0.128*\"learning\" + 0.091*\"machine\" + 0.054*\"industry\" + 0.054*\"technology\" + 0.054*\"deep\"')\n",
            "\n",
            "Document Similarity (cosine): 0.2886751345948129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    }
  ]
}